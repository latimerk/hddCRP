{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import hddCRP.behaviorDataHandlers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2', 'D1', 'D2', 'E1', 'E2', 'F1', 'F2', 'G1', 'G2', 'H1', 'I1', 'I2', 'J1', 'J2']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context_length = 1;\n",
    "num_runs = 1;\n",
    "runs = range(num_runs);\n",
    "num_warmup_samples = 5000\n",
    "num_samples        = 20000\n",
    "overwrite_existing_results = False\n",
    "\n",
    "simple_model = False\n",
    "single_concentration = False\n",
    "sequential_distances_only = True # if false, model setup more as a \"smoother\". If true, can simulate from the model\n",
    "\n",
    "if(sequential_distances_only):\n",
    "    results_directory = \"Results/sequentialModel/\"\n",
    "else:\n",
    "    results_directory = \"Results/\"\n",
    "if(simple_model):\n",
    "    results_directory += \"simple/\"\n",
    "elif(single_concentration):\n",
    "    results_directory += \"singleAlpha/\"\n",
    "\n",
    "if(not os.path.exists(results_directory)):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "data_filename = 'data/Data_turns_all_by_session.pkl';\n",
    "with open(data_filename, 'rb') as data_file:\n",
    "    data = pickle.load(data_file)\n",
    "\n",
    "subjects = list(data[\"data\"].keys())\n",
    "# subjects = [\"A1\"] \n",
    "subjects.sort()\n",
    "print(\"subjects = \" + str(subjects))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN 0\n",
      "subject_idx 0: A1\n",
      "Sample 0 / 25000\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'jump' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m alphas_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha_concentration_no_context\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha_concentration_one_back_context\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha_concentration_two_back_context\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     25\u001b[0m alphas_names \u001b[38;5;241m=\u001b[39m alphas_names[:(context_length\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m---> 26\u001b[0m model, samples, step_size_settings \u001b[38;5;241m=\u001b[39m \u001b[43mhddCRP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbehaviorDataHandlers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_model_for_maze_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_warmup_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_warmup_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_concentration_parameter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msingle_concentration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msimple_model\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m MCMC_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep_size_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m : step_size_settings\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_warmup_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m : num_warmup_samples,\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m : num_samples,\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m : seed, \n\u001b[1;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequential_distances_only\u001b[39m\u001b[38;5;124m\"\u001b[39m : sequential_distances_only}\n\u001b[1;32m     34\u001b[0m samples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtau_parameter_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tau_names\n",
      "File \u001b[0;32m~/gitCode/hddCRP/src/hddCRP/behaviorDataHandlers.py:555\u001b[0m, in \u001b[0;36msample_model_for_maze_data\u001b[0;34m(hddcrp, num_samples, num_warmup_samples, uniform_prior, print_every, prior_shapes, prior_scales, single_concentration_parameter, compute_transition_probabilties)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(compute_transition_probabilties):\n\u001b[1;32m    554\u001b[0m     transition_probabilities[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobabilities\u001b[39m\u001b[38;5;124m\"\u001b[39m][ss,:,:,:] \u001b[38;5;241m=\u001b[39m hddcrp\u001b[38;5;241m.\u001b[39mcompute_preditive_transition_probabilities()\n\u001b[0;32m--> 555\u001b[0m     transition_probabilities[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampled_probabilities\u001b[39m\u001b[38;5;124m\"\u001b[39m][ss,:,:,:] \u001b[38;5;241m=\u001b[39m \u001b[43mhddcrp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_preditive_transition_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(ss \u001b[38;5;241m<\u001b[39m num_warmup_samples):\n\u001b[1;32m    558\u001b[0m     step_size_settings\u001b[38;5;241m.\u001b[39mupdate(np\u001b[38;5;241m.\u001b[39mexp(samples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_acceptance_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m][ss]))\n",
      "File \u001b[0;32m~/gitCode/hddCRP/src/hddCRP/modelFittingSequential.py:644\u001b[0m, in \u001b[0;36msequentialhddCRPModel.sample_preditive_transition_probabilities\u001b[0;34m(self, alphas, weight_params, rng)\u001b[0m\n\u001b[1;32m    641\u001b[0m     pa \u001b[38;5;241m=\u001b[39m alphas[layer] \u001b[38;5;241m/\u001b[39m (alphas[layer] \u001b[38;5;241m+\u001b[39m norm_c)\n\u001b[1;32m    643\u001b[0m     new_jump \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m~\u001b[39mjumped) \u001b[38;5;241m&\u001b[39m (rng\u001b[38;5;241m.\u001b[39mrandom(size\u001b[38;5;241m=\u001b[39mpa\u001b[38;5;241m.\u001b[39msize) \u001b[38;5;241m<\u001b[39m pa)\n\u001b[0;32m--> 644\u001b[0m     jump \u001b[38;5;241m=\u001b[39m \u001b[43mjump\u001b[49m \u001b[38;5;241m|\u001b[39m new_jump\n\u001b[1;32m    646\u001b[0m aa \u001b[38;5;241m=\u001b[39m prob_group_same_obs[:,layer,:,:] \u001b[38;5;241m+\u001b[39m bm;\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_contexts):\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'jump' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "for run_idx in runs:\n",
    "    print(f\"RUN {run_idx}\")\n",
    "    for subject_idx, subject in enumerate(subjects):\n",
    "        print(f\"subject_idx {subject_idx}: {subject}\")\n",
    "        filename = \"{results_directory}/Subject_{subject_id}_context_{context_length}_run_{run_idx}.pkl\".format(results_directory=results_directory, subject_id=subject, run_idx=run_idx, context_length=context_length)\n",
    "        if(not os.path.isfile(filename) or overwrite_existing_results):\n",
    "            # for each run, should do some randomization of initial parameters (with known seeds so we can repeat everything)\n",
    "\n",
    "            seed = subject_idx * 1000 + run_idx;\n",
    "            rng = np.random.Generator(np.random.MT19937(seed))\n",
    "\n",
    "            sequences = data[\"data\"][subject][\"data\"]; # turns in each session\n",
    "            session_types = data[\"data\"][subject][\"task\"] # which maze\n",
    "\n",
    "            ii = session_types.index(\"C\")\n",
    "            sequences = [sequences[ii]]\n",
    "            session_types = [session_types[ii]]\n",
    "\n",
    "\n",
    "            model = hddCRP.behaviorDataHandlers.create_hddCRP(sequences, session_types, rng = rng, \n",
    "                        sequential_distances_only=sequential_distances_only, depth=(context_length+1), include_timescales=(not simple_model))\n",
    "\n",
    "            tau_names = [str(xx) for xx in model.weight_param_labels]\n",
    "            alphas_names = [\"alpha_concentration_no_context\", \"alpha_concentration_one_back_context\", \"alpha_concentration_two_back_context\"]\n",
    "            alphas_names = alphas_names[:(context_length+1)]\n",
    "            model, samples, step_size_settings = hddCRP.behaviorDataHandlers.sample_model_for_maze_data(model, num_samples=num_samples, num_warmup_samples=num_warmup_samples, \n",
    "                print_every=5000, single_concentration_parameter=(single_concentration or simple_model))\n",
    "            \n",
    "            MCMC_info = {\"step_size_settings\" : step_size_settings.to_dict(),\n",
    "                        \"num_warmup_samples\" : num_warmup_samples,\n",
    "                        \"num_samples\" : num_samples,\n",
    "                        \"seed\" : seed, \n",
    "                        \"sequential_distances_only\" : sequential_distances_only}\n",
    "            samples[\"tau_parameter_names\"] = tau_names\n",
    "            samples[\"alphas_names\"] = alphas_names\n",
    "            samples[\"stats\"] = {\"subject\" : subject, \"contexts\" : model._groupings, \"observations\" : model._Y}\n",
    "            \n",
    "            # save results to filename\n",
    "            with open(filename, \"wb\") as results_file:\n",
    "                results_data = {\"MCMC_info\" : MCMC_info,\n",
    "                                \"samples\" : samples}\n",
    "                pickle.dump(results_data, results_file)\n",
    "                print(\"saved\")\n",
    "\n",
    "            raise RuntimeError(\"stopping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for loading results and computing estimates with error bars for a single subject\n",
    "#subject = 'A1'\n",
    "\n",
    "subjects_2 = np.array([True if xx.endswith('2') else False for xx in subjects])\n",
    "subjects_1 = np.array([True if xx.endswith('1') else False for xx in subjects])\n",
    "\n",
    "taus = np.zeros((num_samples,1,len(runs),len(subjects)))\n",
    "alphas = np.zeros((num_samples,context_length+1,len(runs),len(subjects)))\n",
    "\n",
    "for run_idx in runs:\n",
    "    for subject_idx, subject in enumerate(subjects):\n",
    "        filename = \"{results_directory}/Subject_{subject_id}_context_{context_length}_run_{run_idx}.pkl\".format(results_directory=results_directory, subject_id=subject, run_idx=run_idx, context_length=context_length)\n",
    "        with open(filename, \"rb\") as file:\n",
    "            results = pickle.load(file)\n",
    "\n",
    "            ss = range(results[\"MCMC_info\"][\"num_warmup_samples\"], results[\"MCMC_info\"][\"num_warmup_samples\"]+results[\"MCMC_info\"][\"num_samples\"])\n",
    "            taus[:,:,run_idx,subject_idx] = np.exp(results[\"samples\"][\"log_taus\"][ss,:])\n",
    "            alphas[:,:,run_idx,subject_idx] = results[\"samples\"][\"alphas\"][ss,:]\n",
    "\n",
    "        # ci_range = [2.5, 97.5]\n",
    "\n",
    "        # mean_taus = np.mean(taus, axis=0)\n",
    "        # mean_alphas = np.mean(alphas, axis=0)\n",
    "        # std_taus = np.std(taus, axis=0)\n",
    "        # std_alphas = np.std(alphas, axis=0)\n",
    "        # ci95_taus = np.percentile(taus, ci_range, axis=0)\n",
    "        # ci95_alphas = np.percentile(alphas, ci_range, axis=0)\n",
    "\n",
    "        # accepted = results[\"samples\"][\"accepted\"][ss]\n",
    "        # print(\"Fraction of accepted MH samples: \" + str(np.mean(accepted)))\n",
    "\n",
    "    # plt.subplot(1,2,1)\n",
    "    # plt.plot(taus)\n",
    "\n",
    "    # plt.subplot(1,2,2)\n",
    "    # plt.plot(alphas)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.mean(taus[:,0,0,subjects_2],axis=0));\n",
    "# plt.plot(np.mean(taus[:,0,1,subjects_2],axis=0));\n",
    "plt.plot(np.percentile(taus[:,0,0,subjects_2],[50], axis=0).T, 'b-');\n",
    "plt.plot(np.percentile(taus[:,0,0,subjects_2],[5,95], axis=0).T, 'b--');\n",
    "plt.plot(np.percentile(taus[:,0,0,subjects_1],[50], axis=0).T, 'r-');\n",
    "plt.plot(np.percentile(taus[:,0,0,subjects_1],[5,95], axis=0).T, 'r--');\n",
    "# plt.plot(np.percentile(taus[:,0,1,subjects_2],[5, 50, 95], axis=0));\n",
    "# plt.plot(np.median(taus[:,0,0,subjects_1],axis=0));\n",
    "# plt.plot(np.median(taus[:,0,1,subjects_1],axis=0));\n",
    "# plt.plot(np.median(alphas[:,0,0,subjects_2],axis=0));\n",
    "# plt.plot(np.median(alphas[:,0,1,subjects_2],axis=0));\n",
    "# plt.plot(np.median(alphas[:,1,0,subjects_1],axis=0));\n",
    "# plt.plot(np.median(alphas[:,1,1,subjects_1],axis=0));\n",
    "# plt.plot(np.median(alphas[:,1,0,subjects_2],axis=0));\n",
    "# plt.plot(np.median(alphas[:,1,1,subjects_2],axis=0));\n",
    "\n",
    "subjects=np.array(subjects)\n",
    "subjects[subjects_2][6]\n",
    "np.where(subjects=='G2')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ranksums\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "mc_seed = 1003;\n",
    "rng = np.random.Generator(np.random.MT19937(mc_seed))\n",
    "def p_difference(a, b, M = 100000, rng = np.random.default_rng(), prctiles = [2.5,50,97.5]):\n",
    "    aas = np.zeros((M, a.shape[1]))\n",
    "    bbs = np.zeros((M, b.shape[1]))\n",
    "    for kk in range(a.shape[1]):\n",
    "        aas[:,kk] = a[rng.integers(0,a.shape[0],size=(M)),kk]\n",
    "    for kk in range(b.shape[1]):\n",
    "        bbs[:,kk] = b[rng.integers(0,b.shape[0],size=(M)),kk]\n",
    "    aas = np.median(aas,axis=1)\n",
    "    bbs = np.median(bbs,axis=1)\n",
    "    # aas = gmean(aas,axis=1)\n",
    "    # bbs = gmean(bbs,axis=1)\n",
    "    dds = bbs-aas\n",
    "    return np.mean(aas < bbs), np.mean(dds), np.percentile(dds, prctiles)\n",
    "\n",
    "run_idx = 0;\n",
    "alpha_idx = 1;\n",
    "param = taus[:,0,run_idx,:].squeeze()\n",
    "# param = alphas[:,alpha_idx,run_idx,:].squeeze()\n",
    "\n",
    "pps = np.zeros((len(subjects)))\n",
    "mean_diffs = np.zeros((len(subjects)))\n",
    "prctile_diffs = np.zeros((len(subjects), 3))\n",
    "\n",
    "# testing for what happens to stats when leaving out single rat\n",
    "# for ss in range(len(subjects)):\n",
    "#     subjects_1c = np.array(subjects_1);\n",
    "#     subjects_2c = np.array(subjects_2);\n",
    "#     subjects_1c[ss] = False;\n",
    "#     subjects_2c[ss] = False;\n",
    "#     a = param[:,subjects_1c];\n",
    "#     b = param[:,subjects_2c];\n",
    "\n",
    "#     pps[ss], mean_diffs[ss], prctile_diffs[ss,:] = p_difference(a, b, rng=rng)\n",
    "\n",
    "# a = np.median(param[:,subjects_1],axis=0,keepdims=True);\n",
    "# b = np.median(param[:,subjects_2],axis=0,keepdims=True);\n",
    "# b = np.median(param[:,subjects_1],axis=0,keepdims=True);\n",
    "a = param[:,subjects_1];\n",
    "b = param[:,subjects_2];\n",
    "p_all, mean_diff_all, prctile_diffs_all = p_difference(a,b,rng=rng)\n",
    "\n",
    "stats_ranksum, p_ranksum = ranksums(np.median(a,axis=0), np.median(b,axis=0))\n",
    "\n",
    "print(p_ranksum)\n",
    "# print(pps)\n",
    "print(p_all)\n",
    "# print(mean_diff_all)\n",
    "print(prctile_diffs_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def diffs(a, b, M = 100000, rng = np.random.default_rng()):\n",
    "    aas = np.zeros((M, a.shape[1]))\n",
    "    bbs = np.zeros((M, b.shape[1]))\n",
    "    for kk in range(a.shape[1]):\n",
    "        aas[:,kk] = a[rng.integers(0,a.shape[0],size=(M)),kk]\n",
    "    for kk in range(b.shape[1]):\n",
    "        bbs[:,kk] = b[rng.integers(0,b.shape[0],size=(M)),kk]\n",
    "    aas = np.mean(aas,axis=1)\n",
    "    bbs = np.mean(bbs,axis=1)\n",
    "    dds = bbs-aas\n",
    "    return dds\n",
    "    \n",
    "a = param[:,subjects_1];\n",
    "b = param[:,subjects_2];\n",
    "d = diffs(a,b,rng=rng)\n",
    "\n",
    "plt.hist(d,100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[\"transition_probabilities\"].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JaiYuLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
