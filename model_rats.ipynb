{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import hddCRP.behaviorDataHandlers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects = ['A1']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_runs = 4;\n",
    "runs = range(num_runs);\n",
    "num_warmup_samples = 5000\n",
    "num_samples        = 10000\n",
    "overwrite_existing_results = True\n",
    "\n",
    "sequential_distances_only = True # if false, model setup more as a \"smoother\". If true, can simulate from the model\n",
    "\n",
    "if(sequential_distances_only):\n",
    "    results_directory = \"Results/sequentialModel/\"\n",
    "else:\n",
    "    results_directory = \"Results\"\n",
    "if(not os.path.exists(results_directory)):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "data_filename = 'data/Data_turns_all_by_session.pkl';\n",
    "with open(data_filename, 'rb') as data_file:\n",
    "    data = pickle.load(data_file)\n",
    "\n",
    "subjects = list(data[\"data\"].keys())\n",
    "subjects = [\"A1\"]\n",
    "subjects.sort()\n",
    "print(\"subjects = \" + str(subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stopping",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m session_types \u001b[38;5;241m=\u001b[39m [session_types[ii]]\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m hddCRP\u001b[38;5;241m.\u001b[39mbehaviorDataHandlers\u001b[38;5;241m.\u001b[39mcreate_hddCRP(sequences, session_types, rng \u001b[38;5;241m=\u001b[39m rng, sequential_distances_only\u001b[38;5;241m=\u001b[39msequential_distances_only)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstopping\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m tau_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(xx) \u001b[38;5;28;01mfor\u001b[39;00m xx \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mweight_param_labels]\n\u001b[1;32m     21\u001b[0m alphas_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha_concentration_no_context\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha_concentration_one_back_context\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha_concentration_two_back_context\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stopping"
     ]
    }
   ],
   "source": [
    "\n",
    "for run_idx in runs:\n",
    "    for subject_idx, subject in enumerate(subjects):\n",
    "        filename = \"{results_directory}/Subject_{subject_id}_run_{run_idx}.pkl\".format(results_directory=results_directory, subject_id=subject, run_idx=run_idx)\n",
    "        if(not os.path.isfile(filename) or overwrite_existing_results):\n",
    "            # for each run, should do some randomization of initial parameters (with known seeds so we can repeat everything)\n",
    "\n",
    "            seed = subject_idx * 1000 + run_idx;\n",
    "            rng = np.random.Generator(np.random.MT19937(seed))\n",
    "\n",
    "            sequences = data[\"data\"][subject][\"data\"]; # turns in each session\n",
    "            session_types = data[\"data\"][subject][\"task\"] # which maze\n",
    "\n",
    "            ii = session_types.index(\"C\")\n",
    "            sequences = [sequences[ii]]\n",
    "            session_types = [session_types[ii]]\n",
    "\n",
    "\n",
    "            model = hddCRP.behaviorDataHandlers.create_hddCRP(sequences, session_types, rng = rng, sequential_distances_only=sequential_distances_only)\n",
    "            raise RuntimeError(\"stopping\")\n",
    "            tau_names = [str(xx) for xx in model.weight_param_labels]\n",
    "            alphas_names = [\"alpha_concentration_no_context\", \"alpha_concentration_one_back_context\", \"alpha_concentration_two_back_context\"]\n",
    "            model, samples, step_size_settings = hddCRP.behaviorDataHandlers.sample_model_for_maze_data(model, num_samples=num_samples, num_warmup_samples=num_warmup_samples)\n",
    "            \n",
    "            MCMC_info = {\"step_size_settings\" : step_size_settings.to_dict(),\n",
    "                        \"num_warmup_samples\" : num_warmup_samples,\n",
    "                        \"num_samples\" : num_samples,\n",
    "                        \"seed\" : seed, \n",
    "                        \"sequential_distances_only\" : sequential_distances_only}\n",
    "            samples[\"tau_parameter_names\"] = tau_names\n",
    "            samples[\"alphas_names\"] = alphas_names\n",
    "            \n",
    "            # save results to filename\n",
    "            with open(filename, \"wb\") as results_file:\n",
    "                results_data = {\"MCMC_info\" : MCMC_info,\n",
    "                                \"samples\" : samples}\n",
    "                pickle.dump(results_data, results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for loading results and computing estimates with error bars for a single subject\n",
    "subject = 'A1'\n",
    "\n",
    "\n",
    "taus = np.zeros((10000,2,4))\n",
    "alphas = np.zeros((10000,3,4))\n",
    "\n",
    "for run_idx in range(4):\n",
    "    filename = \"{results_directory}/Subject_{subject_id}_run_{run_idx}.pkl\".format(results_directory=results_directory, subject_id=subject, run_idx=run_idx)\n",
    "    with open(filename, \"rb\") as file:\n",
    "        results = pickle.load(file)\n",
    "\n",
    "        ss = range(results[\"MCMC_info\"][\"num_warmup_samples\"], results[\"MCMC_info\"][\"num_warmup_samples\"]+results[\"MCMC_info\"][\"num_samples\"])\n",
    "        taus[:,:,run_idx] = np.exp(results[\"samples\"][\"log_taus\"][ss,:]).squeeze()\n",
    "        alphas[:,:,run_idx] = results[\"samples\"][\"alphas\"][ss,:]\n",
    "\n",
    "        # ci_range = [2.5, 97.5]\n",
    "\n",
    "        # mean_taus = np.mean(taus, axis=0)\n",
    "        # mean_alphas = np.mean(alphas, axis=0)\n",
    "        # std_taus = np.std(taus, axis=0)\n",
    "        # std_alphas = np.std(alphas, axis=0)\n",
    "        # ci95_taus = np.percentile(taus, ci_range, axis=0)\n",
    "        # ci95_alphas = np.percentile(alphas, ci_range, axis=0)\n",
    "\n",
    "        # accepted = results[\"samples\"][\"accepted\"][ss]\n",
    "        # print(\"Fraction of accepted MH samples: \" + str(np.mean(accepted)))\n",
    "\n",
    "    # plt.subplot(1,2,1)\n",
    "    # plt.plot(taus)\n",
    "\n",
    "    # plt.subplot(1,2,2)\n",
    "    # plt.plot(alphas)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['' 'NULL-' 'NULL-NULL-']\n",
      " ['' '2-' 'NULL-2-']\n",
      " ['' '2-' '2-2-']\n",
      " ['' '0-' '2-0-']\n",
      " ['' '0-' '0-0-']]\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['0-', '1-', '2-', '0-', '1-', '2-', '0-', '1-', '2-']\n",
      "['0-0-', '0-1-', '0-2-', '1-0-', '1-1-', '1-2-', '2-0-', '2-1-', '2-2-']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 2, 0, 1, 2, 0, 1, 2]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "print(model._groupings[:5,:])\n",
    "# print(model._Y)\n",
    "\n",
    "prefixes = [str(xx) + '-' for xx in np.unique(model._Y)]\n",
    "combinations = list(product(prefixes, prefixes))\n",
    "\n",
    "level_0_grps = ['' for xx in combinations]\n",
    "level_1_grps = [''.join(xx[-1:]) for xx in combinations]\n",
    "level_2_grps = [''.join(xx[-2:]) for xx in combinations]\n",
    "\n",
    "print(level_0_grps)\n",
    "print(level_1_grps)\n",
    "print(level_2_grps)\n",
    "\n",
    "\n",
    "g_ns_level_0 = [int(model._groupings_compact[model._groupings[:,0] == xx,0][0]) if np.any(np.isin(xx,model._groupings[:,0])) else model.num_groups[0] for xx in level_0_grps]\n",
    "g_ns_level_1 = [int(model._groupings_compact[model._groupings[:,1] == xx,1][0]) if np.any(np.isin(xx,model._groupings[:,1])) else model.num_groups[1] for xx in level_1_grps]\n",
    "g_ns_level_2 = [int(model._groupings_compact[model._groupings[:,2] == xx,2][0]) if np.any(np.isin(xx,model._groupings[:,2])) else model.num_groups[2] for xx in level_2_grps]\n",
    "\n",
    "gnums = [g_ns_level_0, g_ns_level_1, g_ns_level_2]\n",
    "print(g_ns_level_0)\n",
    "print(g_ns_level_1)\n",
    "print(g_ns_level_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07098156 0.07468594 0.07858365 0.08268478 0.08699994 0.09154029\n",
      "  0.0963176  0.10134422 0.10663318 0.11219815 0.11805355 0.12421454\n",
      "  0.13069705 0.13751787 0.14469465 0.15224598 0.1601914  0.16855147\n",
      "  0.17734784 0.18660328 0.19634173 0.20658842 0.21736986 0.22871397\n",
      "  0.2406501  0.25320915 0.26642364 0.28032776 0.29495752 0.31035077\n",
      "  0.32654736 0.34358922 0.36152047 0.38038751 0.40023918 0.42112687\n",
      "  0.44310465 0.46622941 0.490561   0.51616241 0.54309991 0.57144323\n",
      "  0.60126572 0.63264459 0.66566106 0.7004006  0.73695313 0.77541326\n",
      "  0.81588055 0.85845974 0.90326106 0.95040047]]\n"
     ]
    }
   ],
   "source": [
    "from hddCRP.modelFitting import complete_exponential_distance_function_for_maze_task, hddCRPModel\n",
    "\n",
    "\n",
    "obs_idx = model.N-1\n",
    "n_obs = np.size(obs_idx)\n",
    "\n",
    "D_new = np.array(model._D[obs_idx,:,:].reshape((n_obs,model.N,1)))\n",
    "inds_new = model._weight_function_setup[\"inds\"][obs_idx,:].reshape((n_obs,model.N))\n",
    "D_new[inds_new == 0] += 1\n",
    "\n",
    "weights = complete_exponential_distance_function_for_maze_task(D_new, model.weight_params, inds_new, model._weight_function_setup[\"timescale_inds\"],model._weight_function_setup[\"constant_scale_inds\"])\n",
    "#D_new.shape\n",
    "# inds_new\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3879662 , 0.26683665, 0.34519715],\n",
       "       [0.40702903, 0.3007966 , 0.29217437],\n",
       "       [0.54159123, 0.26896808, 0.18944069],\n",
       "       [0.40292722, 0.27758304, 0.31948974],\n",
       "       [0.39086562, 0.26949936, 0.33963502],\n",
       "       [0.55544724, 0.23774744, 0.20680532],\n",
       "       [0.41164383, 0.22013184, 0.36822433],\n",
       "       [0.37609019, 0.33948327, 0.28442653],\n",
       "       [0.51590451, 0.25889495, 0.22520054]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3879662 , 0.26683665, 0.34519715],\n",
       "       [0.40702903, 0.3007966 , 0.29217437],\n",
       "       [0.54159123, 0.26896808, 0.18944069],\n",
       "       [0.40292722, 0.27758304, 0.31948974],\n",
       "       [0.39086562, 0.26949936, 0.33963502],\n",
       "       [0.55544724, 0.23774744, 0.20680532],\n",
       "       [0.41164383, 0.22013184, 0.36822433],\n",
       "       [0.37609019, 0.33948327, 0.28442653],\n",
       "       [0.51590451, 0.25889495, 0.22520054]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JaiYuLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
