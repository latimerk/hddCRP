{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hddCRP.simulations\n",
    "import hddCRP.modelFitting\n",
    "import hddCRP.behaviorDataHandlers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_directory = \"Results/Simulations\"\n",
    "if(not os.path.exists(results_directory)):\n",
    "    os.makedirs(results_directory)\n",
    "    \n",
    "simulation_id = 3; # for naming file\n",
    "\n",
    "overwrite_existing_results = True\n",
    "\n",
    "num_runs = 5;\n",
    "num_warmup_samples = 2000\n",
    "num_samples = 3000\n",
    "\n",
    "initialize_fit_with_real_connections = False;\n",
    "\n",
    "if(simulation_id == 0):\n",
    "    depth  = 3; # look 2 actions in the past\n",
    "    alphas = [2,10,20] # concentration parameters: per depth in the hddCRP tree. alphas[0] first level (no action context), alphas[1] is the second (for regularizing p(y_t | y_{t-1})), etc...\n",
    "    between_session_time_constants = np.array([[ 1, 2],\n",
    "                                                [2, 1]]) # units = sessions\n",
    "    within_session_time_constant = [120,80] # units = actions\n",
    "    session_length = lambda run_idx : 60 # trials per session\n",
    "    action_labels = [0,1,2] \n",
    "    maze_symbols = ['A', 'B']\n",
    "    uniform_prior = True\n",
    "    min_blocks_per_type = 2\n",
    "    max_blocks_per_type = 10;\n",
    "elif(simulation_id == 1):\n",
    "    depth  = 3; # look 2 actions in the past\n",
    "    alphas = [2,10,20] # concentration parameters: per depth in the hddCRP tree. alphas[0] first level (no action context), alphas[1] is the second (for regularizing p(y_t | y_{t-1})), etc...\n",
    "    between_session_time_constants = np.array([[ 1]]) # units = sessions\n",
    "    within_session_time_constant = [100] # units = actions\n",
    "    session_length = lambda run_idx : 60 # trials per session\n",
    "    action_labels = [0,1,2] \n",
    "    maze_symbols = ['A']\n",
    "    uniform_prior = True\n",
    "    min_blocks_per_type = 2\n",
    "    max_blocks_per_type = 10;\n",
    "elif(simulation_id == 2):\n",
    "    depth  = 1; # look 2 actions in the past\n",
    "    alphas = [2] # concentration parameters: per depth in the hddCRP tree. alphas[0] first level (no action context), alphas[1] is the second (for regularizing p(y_t | y_{t-1})), etc...\n",
    "    between_session_time_constants = np.array([[ 1]]) # units = sessions\n",
    "    within_session_time_constant = [100] # units = actions\n",
    "    session_length = lambda run_idx : 100 * (1+run_idx) # trials per session\n",
    "    action_labels = [0,1,2] \n",
    "    maze_symbols = ['A']\n",
    "    uniform_prior = True\n",
    "    min_blocks_per_type = 1\n",
    "    max_blocks_per_type = 1;\n",
    "elif(simulation_id == 3):\n",
    "    depth  = 1; # look 2 actions in the past\n",
    "    alphas = [2] # concentration parameters: per depth in the hddCRP tree. alphas[0] first level (no action context), alphas[1] is the second (for regularizing p(y_t | y_{t-1})), etc...\n",
    "    between_session_time_constants = np.array([[ 1]]) # units = sessions\n",
    "    within_session_time_constant = [100] # units = actions\n",
    "    session_length = lambda run_idx : 100 * (1+run_idx) # trials per session\n",
    "    action_labels = [0,1,2] \n",
    "    maze_symbols = ['A']\n",
    "    uniform_prior = False\n",
    "    min_blocks_per_type = 1\n",
    "    max_blocks_per_type = 1;\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "true_parameters = {}\n",
    "alpha_strs = [\"no\", \"one_back\", \"two_back\", \"three_back\"]\n",
    "for dd in range(depth):\n",
    "    true_parameters[\"alpha_concentration_\" + alpha_strs[dd] + \"_context\"] = alphas[dd]\n",
    "for aa_i, aa in enumerate(maze_symbols): \n",
    "    true_parameters[\"within_session_\" + aa + \"_time_constant\"] = within_session_time_constant[aa_i]\n",
    "\n",
    "if(max_blocks_per_type > 1):\n",
    "    for aa_i, aa in enumerate(maze_symbols):\n",
    "        for bb_i, bb in enumerate(maze_symbols[aa_i:]):\n",
    "            true_parameters[aa + \"_to_\" + bb + \"_session_time_constant\"] = between_session_time_constants[aa_i,bb_i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 / 5000\n",
      "Sample 2500 / 5000\n",
      "Sample 0 / 5000\n",
      "Sample 2500 / 5000\n",
      "Sample 0 / 5000\n",
      "Sample 2500 / 5000\n",
      "Sample 0 / 5000\n",
      "Sample 2500 / 5000\n",
      "Sample 0 / 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/latimerk/gitCode/hddCRP/src/hddCRP/modelFitting.py:1291: RuntimeWarning: divide by zero encountered in log\n",
      "  log_P_cons += np.sum(np.log(w_c))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2500 / 5000\n"
     ]
    }
   ],
   "source": [
    "for run_idx in range(num_runs):\n",
    "    for N_blocks_per_type in range(min_blocks_per_type,max_blocks_per_type+1):\n",
    "        filename = \"{results_directory}/Sim_{sim_num}_size_{N_blocks_per_type}_run_{run_idx}.pkl\".format(results_directory=results_directory, sim_num=simulation_id, N_blocks_per_type=N_blocks_per_type, run_idx=run_idx)\n",
    "        if(not os.path.isfile(filename) or overwrite_existing_results):\n",
    "            rng_seed_sim = N_blocks_per_type + 100 + 10000*run_idx;\n",
    "            rng_seed_fit = N_blocks_per_type + 200 + 10000*run_idx;\n",
    "            rng_sim = np.random.Generator(np.random.MT19937(rng_seed_sim))\n",
    "            rng_fit = np.random.Generator(np.random.MT19937(rng_seed_fit))\n",
    "\n",
    "            session_lengths = [session_length(run_idx)] * (len(maze_symbols) * N_blocks_per_type)\n",
    "            session_labels = [];\n",
    "            for aa in maze_symbols:\n",
    "                session_labels += [aa] * N_blocks_per_type  # which maze\n",
    "\n",
    "            seqs, connection_data = hddCRP.simulations.simulate_sequential_hddCRP(session_lengths, session_labels, action_labels, depth, rng_sim, alphas, \n",
    "                    between_session_time_constants = between_session_time_constants, within_session_time_constant = within_session_time_constant)\n",
    "\n",
    "            simulation_info = {\"rng_seed_simulation\" : rng_seed_sim, \"rng_seed_fitting\" : rng_seed_fit, \"rng_type\" : \"MT19937\",\n",
    "                            \"session_lengths\" : session_lengths, \"session_labels\" : session_labels, \"action_labels\" : action_labels,\n",
    "                            \"seqs\" : seqs, \"connection_data\" : connection_data}\n",
    "\n",
    "            model = hddCRP.simulations.create_hddCRPModel_from_simulated_sequential_hddCRP(seqs, connection_data, rng=rng_fit, use_real_connections=initialize_fit_with_real_connections)\n",
    "            \n",
    "            tau_names = [str(xx) for xx in model.weight_param_labels]\n",
    "            alphas_names = [\"alpha_concentration_no_context\", \"alpha_concentration_one_back_context\", \"alpha_concentration_two_back_context\"]\n",
    "            model, samples, step_size_settings = hddCRP.behaviorDataHandlers.sample_model_for_maze_data(model, num_samples=num_samples, \n",
    "                            num_warmup_samples=num_warmup_samples, print_every=2500, uniform_prior=uniform_prior)\n",
    "\n",
    "            MCMC_info = {\"initialized_with_true_connections\" : initialize_fit_with_real_connections,\n",
    "                        \"step_size_settings\" : step_size_settings.to_dict(),\n",
    "                        \"num_warmup_samples\" : num_warmup_samples,\n",
    "                        \"num_samples\" : num_samples,\n",
    "                        \"uniform_prior\" : uniform_prior}\n",
    "            samples[\"tau_parameter_names\"] = tau_names\n",
    "            samples[\"alphas_names\"] = alphas_names[:depth]\n",
    "            \n",
    "            # save results to filename\n",
    "            with open(filename, \"wb\") as results_file:\n",
    "                results_data = {\"true_parameters\" : true_parameters,\n",
    "                                \"simulation_info\" : simulation_info,\n",
    "                                \"MCMC_info\" : MCMC_info,\n",
    "                                \"samples\" : samples}\n",
    "                pickle.dump(results_data, results_file)\n",
    "            # \n",
    "            #   true_parameters\n",
    "            #   simulation_info\n",
    "            #   MCMC_info\n",
    "            #   samples\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metropolis-Hastings acceptance rate and step size:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mloc[:,(p_name,N_blocks_per_type,run_idx)] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_taus\u001b[39m\u001b[38;5;124m\"\u001b[39m][s_index,ii])\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii, p_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malphas_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] ):\n\u001b[0;32m---> 66\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mloc[:,(p_name,N_blocks_per_type,run_idx)] \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msamples\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malphas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43mii\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(run_idx) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, num blocks \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(N_blocks_per_type) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: rate = \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccepted\u001b[39m\u001b[38;5;124m\"\u001b[39m][s_index])) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, step size = \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39msqrt(results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMCMC_info\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep_size_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep_size_fixed\u001b[39m\u001b[38;5;124m\"\u001b[39m])))\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "empty_data = np.zeros((num_samples, num_runs*len(true_parameters)*(max_blocks_per_type-1)))\n",
    "empty_data.fill(np.nan)\n",
    "parameters = pd.DataFrame(empty_data,\n",
    "                  columns=pd.MultiIndex.from_product([true_parameters.keys(), range(2,max_blocks_per_type+1), range(num_runs)], names=[\"parameter\", \"sessions per maze\", \"run\"]))\n",
    "parameters.index.name='sample'\n",
    "\n",
    "# plot results: MCMC credible intervals for each parameter as a function of N_blocks_per_type\n",
    "print(\"Metropolis-Hastings acceptance rate and step size:\")\n",
    "for run_idx in range(num_runs):\n",
    "    for N_blocks_per_type in range(min_blocks_per_type,max_blocks_per_type+1):\n",
    "        filename = \"{results_directory}/Sim_{sim_num}_size_{N_blocks_per_type}_run_{run_idx}.pkl\".format(results_directory=results_directory, sim_num=simulation_id, N_blocks_per_type=N_blocks_per_type, run_idx=run_idx)\n",
    "        if(not os.path.isfile(filename)):\n",
    "            raise RuntimeError(\"Results file not found: \" + filename)\n",
    "            # throw error\n",
    "        \n",
    "        # load file\n",
    "        with open(filename, \"rb\") as results_file:\n",
    "            results = pickle.load(results_file)\n",
    "\n",
    "            '''\n",
    "            Make sure all the files loaded are correct\n",
    "            '''\n",
    "\n",
    "            # check for correct results fields\n",
    "            expected_fields = [\"true_parameters\", \"simulation_info\", \"MCMC_info\", \"samples\"]\n",
    "            if(not isinstance(results, dict) or not np.all(np.isin(expected_fields, list(results.keys())))):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected results fields\")\n",
    "            \n",
    "            session_lengths = [session_length(run_idx)] * (len(maze_symbols) * N_blocks_per_type)\n",
    "            session_labels = [];\n",
    "            for aa in maze_symbols:\n",
    "                session_labels += [aa] * N_blocks_per_type  # which maze\n",
    "            if(results[\"simulation_info\"][\"session_lengths\"] != session_lengths):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected session lengths\")\n",
    "            if(results[\"simulation_info\"][\"session_labels\"] != session_labels):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected session labels\")\n",
    "            if(results[\"simulation_info\"][\"action_labels\"] != action_labels):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected action labels\")\n",
    "\n",
    "\n",
    "            results[\"samples\"][\"alphas_names\"] = results[\"samples\"][\"alphas_names\"][:depth]\n",
    "\n",
    "            # check if samples matches (if greater than expected, raise a warning; it less, raise an error)\n",
    "            expected_fields_samples = [\"log_taus\", \"alphas\", \"accepted\", \"num_warmup_samples\", \"tau_parameter_names\", \"alphas_names\"]\n",
    "            if(not isinstance(results[\"samples\"], dict) or not np.all(np.isin(expected_fields_samples, list(results[\"samples\"].keys())))):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected results['samples'] fields\")\n",
    "            \n",
    "            num_samples_found = min(results[\"samples\"][\"log_taus\"].shape[0], results[\"samples\"][\"alphas\"].shape[0]) - results[\"samples\"][\"num_warmup_samples\"]\n",
    "            if(num_samples_found < num_samples):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected number of samples in results['samples']. Found \" + str(num_samples_found) + \". Expected \" + str(num_samples) + \".\")\n",
    "\n",
    "            # check if true params match, if not, raise error\n",
    "            if(not np.all(np.isin(list(true_parameters.keys()), results[\"samples\"][\"tau_parameter_names\"] + results[\"samples\"][\"alphas_names\"]))):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected parameters in results['samples']\")\n",
    "            if(not np.all(np.isin(list(true_parameters.keys()), list(results[\"true_parameters\"].keys()))) or not np.all(np.isin(list(results[\"true_parameters\"].keys()), list(true_parameters.keys())))):\n",
    "               raise ValueError(\"File \" + filename + \" does not contain expected parameters in results['true_parameters']\")\n",
    "            \n",
    "            for p_name in true_parameters.keys():\n",
    "               if(true_parameters[p_name] != results[\"true_parameters\"][p_name]):\n",
    "                   raise ValueError(\"File \" + filename + \" does not contain expected parameter value results['true_parameters'][\" + p_name + \"]\")\n",
    "\n",
    "            # load samples from each parameter into dataframe (multi-index: name of param and run number)\n",
    "            s_index = range(results[\"samples\"][\"num_warmup_samples\"], results[\"samples\"][\"num_warmup_samples\"] + num_samples)\n",
    "            for ii, p_name in enumerate(results[\"samples\"][\"tau_parameter_names\"] ):\n",
    "                \n",
    "                parameters.loc[:,(p_name,N_blocks_per_type,run_idx)] = np.exp(results[\"samples\"][\"log_taus\"][s_index,ii])\n",
    "            for ii, p_name in enumerate(results[\"samples\"][\"alphas_names\"] ):\n",
    "                parameters.loc[:,(p_name,N_blocks_per_type,run_idx)] = results[\"samples\"][\"alphas\"][s_index,ii]\n",
    "\n",
    "            print(\"Run \" + str(run_idx) + \", num blocks \" + str(N_blocks_per_type) + \": rate = \" + str(np.mean(results[\"samples\"][\"accepted\"][s_index])) + \", step size = \" + str(np.sqrt(results[\"MCMC_info\"][\"step_size_settings\"][\"step_size_fixed\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_s = parameters.stack([1,2]).reset_index()    \n",
    "n_cols = 2\n",
    "n_rows = int(np.ceil(len(true_parameters)/float(n_cols)))\n",
    "plt.figure(figsize=(n_cols*8,n_rows*4))\n",
    "\n",
    "print(len(true_parameters))\n",
    "for ii, p_name in enumerate(true_parameters.keys()):\n",
    "    plt.subplot(n_rows, n_cols, ii+1)\n",
    "    # for each parameter in the data frame, make a plot of 95% CI and mean over time vs. true params\n",
    "    pp = sns.lineplot(x=\"sessions per maze\", y=p_name,\n",
    "                hue=\"run\", errorbar=(\"pi\",95),\n",
    "                data=parameters_s)\n",
    "    plt.plot([2,max_blocks_per_type], [true_parameters[p_name], true_parameters[p_name]], \"k:\")\n",
    "    pp.set_xticks(range(2,max_blocks_per_type+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(np.array(parameters[\"within_session_time_constant\"][2][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alpha_concentration_no_context',\n",
       " 'alpha_concentration_one_back_context',\n",
       " 'alpha_concentration_two_back_context']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"samples\"][\"alphas_names\"]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa7646fadbe57c277d76ff36de8f181b6360ae43924dda8e36f9c735fd43eb1d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit ('JaiYuLab': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
