{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hddCRP.simulations\n",
    "import hddCRP.modelFitting\n",
    "import hddCRP.behaviorDataHandlers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_directory = \"Results/Simulations\"\n",
    "if(not os.path.exists(results_directory)):\n",
    "    os.makedirs(results_directory)\n",
    "    \n",
    "simulation_id = 1; # for naming file\n",
    "\n",
    "overwrite_existing_results = False\n",
    "\n",
    "run_range = range(0,1)\n",
    "\n",
    "num_warmup_samples = 10000\n",
    "num_samples = 40000\n",
    "\n",
    "initialize_fit_with_real_connections = False;\n",
    "\n",
    "prior_scales = None\n",
    "prior_shapes = None\n",
    "\n",
    "if(simulation_id == 0):\n",
    "    depth  = 3; # look 2 actions in the past\n",
    "    alphas = [2,20,20] # concentration parameters: per depth in the hddCRP tree. alphas[0] first level (no action context), alphas[1] is the second (for regularizing p(y_t | y_{t-1})), etc...\n",
    "    between_session_time_constants = np.array([[ 1]]) # units = sessions\n",
    "    within_session_time_constant = [60] # units = actions\n",
    "    session_length = lambda run_idx, block_idx : 50 # trials per session\n",
    "    num_sessions   = lambda run_idx, block_idx : block_idx # trials per session\n",
    "\n",
    "    action_labels = [0,1,2] \n",
    "    maze_symbols = ['A']\n",
    "    uniform_prior = False\n",
    "    min_blocks_per_type = 2\n",
    "    max_blocks_per_type = 10;\n",
    "    prior_scales = {\"alpha\" : 10, \"tau_within\" : 25, \"tau_between\" : 10}\n",
    "    prior_shapes = {\"alpha\" : 2, \"tau_within\" :  2, \"tau_between\" : 2}\n",
    "\n",
    "elif(simulation_id == 1):\n",
    "    depth  = 3; # look 2 actions in the past\n",
    "    alphas = [2,10,20] # concentration parameters: per depth in the hddCRP tree. alphas[0] first level (no action context), alphas[1] is the second (for regularizing p(y_t | y_{t-1})), etc...\n",
    "    between_session_time_constants = np.array([[ 1]]) # units = sessions\n",
    "    within_session_time_constant = [30] # units = actions\n",
    "    session_length = lambda run_idx, block_idx : 50 # trials per session\n",
    "    num_sessions   = lambda run_idx, block_idx : block_idx # trials per session\n",
    "    \n",
    "    action_labels = [0,1,2] \n",
    "    maze_symbols = ['A']\n",
    "    uniform_prior = False\n",
    "    min_blocks_per_type = 2\n",
    "    max_blocks_per_type = 10;\n",
    "    prior_scales = {\"alpha\" : 10, \"tau_within\" : 25, \"tau_between\" : 10}\n",
    "    prior_shapes = {\"alpha\" : 2, \"tau_within\" :  2, \"tau_between\" : 2}\n",
    "\n",
    "elif(simulation_id == 2):\n",
    "    depth  = 3; # look 2 actions in the past\n",
    "    alphas = [2,20,20] # concentration parameters: per depth in the hddCRP tree. alphas[0] first level (no action context), alphas[1] is the second (for regularizing p(y_t | y_{t-1})), etc...\n",
    "    between_session_time_constants = np.array([[ 1]]) # units = sessions\n",
    "    within_session_time_constant = [60] # units = actions\n",
    "    session_length = lambda run_idx, block_idx : 50*block_idx # trials per session\n",
    "    num_sessions   = lambda run_idx, block_idx : 1 # trials per session\n",
    "\n",
    "    action_labels = [0,1,2] \n",
    "    maze_symbols = ['A']\n",
    "    uniform_prior = False\n",
    "    min_blocks_per_type = 1\n",
    "    max_blocks_per_type = 1;\n",
    "    prior_scales = {\"alpha\" : 10, \"tau_within\" : 25, \"tau_between\" : 10}\n",
    "    prior_shapes = {\"alpha\" : 2, \"tau_within\" :  2, \"tau_between\" : 2}\n",
    "elif(simulation_id == 3):\n",
    "    depth  = 3; # look 2 actions in the past\n",
    "    alphas = [2,10,20] # concentration parameters: per depth in the hddCRP tree. alphas[0] first level (no action context), alphas[1] is the second (for regularizing p(y_t | y_{t-1})), etc...\n",
    "    between_session_time_constants = np.array([[ 1]]) # units = sessions\n",
    "    within_session_time_constant = [30] # units = actions\n",
    "    session_length = lambda run_idx, block_idx : 50*block_idx # trials per session\n",
    "    num_sessions   = lambda run_idx, block_idx : 1 # trials per session\n",
    "\n",
    "    action_labels = [0,1,2] \n",
    "    maze_symbols = ['A']\n",
    "    uniform_prior = False\n",
    "    min_blocks_per_type = 1\n",
    "    max_blocks_per_type = 1;\n",
    "    prior_scales = {\"alpha\" : 10, \"tau_within\" : 25, \"tau_between\" : 10}\n",
    "    prior_shapes = {\"alpha\" : 2, \"tau_within\" :  2, \"tau_between\" : 2}\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "true_parameters = {}\n",
    "alpha_strs = [\"no\", \"one_back\", \"two_back\", \"three_back\"]\n",
    "for dd in range(depth):\n",
    "    true_parameters[\"alpha_concentration_\" + alpha_strs[dd] + \"_context\"] = alphas[dd]\n",
    "for aa_i, aa in enumerate(maze_symbols): \n",
    "    true_parameters[\"within_session_\" + aa + \"_time_constant\"] = within_session_time_constant[aa_i]\n",
    "\n",
    "if(max_blocks_per_type > 1):\n",
    "    for aa_i, aa in enumerate(maze_symbols):\n",
    "        for bb_i, bb in enumerate(maze_symbols[aa_i:]):\n",
    "            true_parameters[aa + \"_to_\" + bb + \"_session_time_constant\"] = between_session_time_constants[aa_i,bb_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_idx in run_range:\n",
    "    for N_blocks_per_type in range(min_blocks_per_type,max_blocks_per_type+1):\n",
    "        filename = \"{results_directory}/Sim_{sim_num}_size_{N_blocks_per_type}_run_{run_idx}.pkl\".format(results_directory=results_directory, sim_num=simulation_id, N_blocks_per_type=N_blocks_per_type, run_idx=run_idx)\n",
    "        if(not os.path.isfile(filename) or overwrite_existing_results):\n",
    "            rng_seed_sim = N_blocks_per_type + 100;# + 10000*run_idx;\n",
    "            rng_seed_fit = N_blocks_per_type + 200 + 10000*run_idx;\n",
    "            rng_sim = np.random.Generator(np.random.MT19937(rng_seed_sim))\n",
    "            rng_fit = np.random.Generator(np.random.MT19937(rng_seed_fit))\n",
    "\n",
    "            num_sessions = 1\n",
    "\n",
    "            session_lengths = [session_length(run_idx)] * (len(maze_symbols) * N_blocks_per_type)\n",
    "            session_labels = [];\n",
    "            for aa in maze_symbols:\n",
    "                session_labels += [aa] * N_blocks_per_type  # which maze\n",
    "\n",
    "            seqs, connection_data = hddCRP.simulations.simulate_sequential_hddCRP(session_lengths, session_labels, action_labels, depth, rng_sim, alphas, \n",
    "                    between_session_time_constants = between_session_time_constants, within_session_time_constant = within_session_time_constant)\n",
    "\n",
    "            simulation_info = {\"rng_seed_simulation\" : rng_seed_sim, \"rng_seed_fitting\" : rng_seed_fit, \"rng_type\" : \"MT19937\",\n",
    "                            \"session_lengths\" : session_lengths, \"session_labels\" : session_labels, \"action_labels\" : action_labels,\n",
    "                            \"seqs\" : seqs, \"connection_data\" : connection_data}\n",
    "\n",
    "            model = hddCRP.simulations.create_hddCRPModel_from_simulated_sequential_hddCRP(seqs, connection_data, rng=rng_fit, use_real_connections=initialize_fit_with_real_connections)\n",
    "            \n",
    "            tau_names = [str(xx) for xx in model.weight_param_labels]\n",
    "            alphas_names = [\"alpha_concentration_no_context\", \"alpha_concentration_one_back_context\", \"alpha_concentration_two_back_context\"]\n",
    "            model, samples, step_size_settings = hddCRP.behaviorDataHandlers.sample_model_for_maze_data(model, num_samples=num_samples, \n",
    "                            num_warmup_samples=num_warmup_samples, print_every=2500, uniform_prior=uniform_prior, prior_shapes=prior_shapes, prior_scales=prior_scales)\n",
    "\n",
    "            MCMC_info = {\"initialized_with_true_connections\" : initialize_fit_with_real_connections,\n",
    "                        \"step_size_settings\" : step_size_settings.to_dict(),\n",
    "                        \"num_warmup_samples\" : num_warmup_samples,\n",
    "                        \"num_samples\" : num_samples,\n",
    "                        \"uniform_prior\" : uniform_prior}\n",
    "            samples[\"tau_parameter_names\"] = tau_names\n",
    "            samples[\"alphas_names\"] = alphas_names[:depth]\n",
    "            \n",
    "            # save results to filename\n",
    "            with open(filename, \"wb\") as results_file:\n",
    "                results_data = {\"true_parameters\" : true_parameters,\n",
    "                                \"simulation_info\" : simulation_info,\n",
    "                                \"MCMC_info\" : MCMC_info,\n",
    "                                \"samples\" : samples}\n",
    "                pickle.dump(results_data, results_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_data = np.zeros((num_samples, len(run_range)*len(true_parameters)*(max_blocks_per_type-1)))\n",
    "empty_data.fill(np.nan)\n",
    "parameters = pd.DataFrame(empty_data,\n",
    "                  columns=pd.MultiIndex.from_product([true_parameters.keys(), range(min_blocks_per_type,max_blocks_per_type+1), run_range], names=[\"parameter\", \"sessions per maze\", \"run\"]))\n",
    "parameters.index.name='sample'\n",
    "\n",
    "# plot results: MCMC credible intervals for each parameter as a function of N_blocks_per_type\n",
    "print(\"Metropolis-Hastings acceptance rate and step size:\")\n",
    "for run_idx, run_num in enumerate(run_range):\n",
    "    for N_blocks_per_type in range(min_blocks_per_type,max_blocks_per_type+1):\n",
    "        filename = \"{results_directory}/Sim_{sim_num}_size_{N_blocks_per_type}_run_{run_num}.pkl\".format(results_directory=results_directory, sim_num=simulation_id, N_blocks_per_type=N_blocks_per_type, run_num=run_num)\n",
    "        if(not os.path.isfile(filename)):\n",
    "            raise RuntimeError(\"Results file not found: \" + filename)\n",
    "            # throw error\n",
    "        \n",
    "        # load file\n",
    "        with open(filename, \"rb\") as results_file:\n",
    "            results = pickle.load(results_file)\n",
    "\n",
    "            '''\n",
    "            Make sure all the files loaded are correct\n",
    "            '''\n",
    "\n",
    "            # check for correct results fields\n",
    "            expected_fields = [\"true_parameters\", \"simulation_info\", \"MCMC_info\", \"samples\"]\n",
    "            if(not isinstance(results, dict) or not np.all(np.isin(expected_fields, list(results.keys())))):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected results fields\")\n",
    "            \n",
    "            session_lengths = [session_length(run_idx)] * (len(maze_symbols) * N_blocks_per_type)\n",
    "            session_labels = [];\n",
    "            for aa in maze_symbols:\n",
    "                session_labels += [aa] * N_blocks_per_type  # which maze\n",
    "            if(results[\"simulation_info\"][\"session_lengths\"] != session_lengths):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected session lengths\")\n",
    "            if(results[\"simulation_info\"][\"session_labels\"] != session_labels):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected session labels\")\n",
    "            if(results[\"simulation_info\"][\"action_labels\"] != action_labels):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected action labels\")\n",
    "\n",
    "\n",
    "            #results[\"samples\"][\"alphas_names\"] = results[\"samples\"][\"alphas_names\"][:depth]\n",
    "\n",
    "            # check if samples matches (if greater than expected, raise a warning; it less, raise an error)\n",
    "            expected_fields_samples = [\"log_taus\", \"alphas\", \"accepted\", \"num_warmup_samples\", \"tau_parameter_names\", \"alphas_names\"]\n",
    "            if(not isinstance(results[\"samples\"], dict) or not np.all(np.isin(expected_fields_samples, list(results[\"samples\"].keys())))):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected results['samples'] fields\")\n",
    "            \n",
    "            num_samples_found = min(results[\"samples\"][\"log_taus\"].shape[0], results[\"samples\"][\"alphas\"].shape[0]) - results[\"samples\"][\"num_warmup_samples\"]\n",
    "            if(num_samples_found < num_samples):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected number of samples in results['samples']. Found \" + str(num_samples_found) + \". Expected \" + str(num_samples) + \".\")\n",
    "\n",
    "            # check if true params match, if not, raise error\n",
    "            if(not np.all(np.isin(list(true_parameters.keys()), results[\"samples\"][\"tau_parameter_names\"] + results[\"samples\"][\"alphas_names\"]))):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected parameters in results['samples']\")\n",
    "            if(not np.all(np.isin(list(true_parameters.keys()), list(results[\"true_parameters\"].keys()))) or not np.all(np.isin(list(results[\"true_parameters\"].keys()), list(true_parameters.keys())))):\n",
    "               raise ValueError(\"File \" + filename + \" does not contain expected parameters in results['true_parameters']\")\n",
    "            \n",
    "            for p_name in true_parameters.keys():\n",
    "               if(true_parameters[p_name] != results[\"true_parameters\"][p_name]):\n",
    "                   raise ValueError(\"File \" + filename + \" does not contain expected parameter value results['true_parameters'][\" + p_name + \"]\")\n",
    "\n",
    "            # load samples from each parameter into dataframe (multi-index: name of param and run number)\n",
    "            s_index = range(results[\"samples\"][\"num_warmup_samples\"], results[\"samples\"][\"num_warmup_samples\"] + num_samples)\n",
    "            for ii, p_name in enumerate(results[\"samples\"][\"tau_parameter_names\"] ):\n",
    "                \n",
    "                parameters.loc[:,(p_name,N_blocks_per_type,run_idx)] = np.exp(results[\"samples\"][\"log_taus\"][s_index,ii])\n",
    "            for ii, p_name in enumerate(results[\"samples\"][\"alphas_names\"] ):\n",
    "                parameters.loc[:,(p_name,N_blocks_per_type,run_idx)] = results[\"samples\"][\"alphas\"][s_index,ii]\n",
    "\n",
    "            print(\"Run \" + str(run_idx) + \", num blocks \" + str(N_blocks_per_type) + \": rate = \" + str(np.mean(results[\"samples\"][\"accepted\"][s_index])) + \", step size = \" + str(np.sqrt(results[\"MCMC_info\"][\"step_size_settings\"][\"step_size_fixed\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_s = parameters.stack([1,2]).reset_index()    \n",
    "n_cols = 2\n",
    "n_rows = int(np.ceil(len(true_parameters)/float(n_cols)))\n",
    "plt.figure(figsize=(n_cols*8,n_rows*4))\n",
    "\n",
    "for ii, p_name in enumerate(true_parameters.keys()):\n",
    "    plt.subplot(n_rows, n_cols, ii+1)\n",
    "    # for each parameter in the data frame, make a plot of 95% CI and mean over time vs. true params\n",
    "    pp = sns.lineplot(x=\"sessions per maze\", y=p_name,\n",
    "                hue=\"run\", errorbar=(\"pi\",95),\n",
    "                data=parameters_s)\n",
    "    plt.plot([min_blocks_per_type,max_blocks_per_type], [true_parameters[p_name], true_parameters[p_name]], \"k:\")\n",
    "    pp.set_xticks(range(min_blocks_per_type,max_blocks_per_type+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(np.array(parameters[\"within_session_time_constant\"][2][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results[\"samples\"][\"alphas\"])\n",
    "plt.plot(np.exp(results[\"samples\"][\"log_taus\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(results[\"samples\"][\"alphas\"],np.exp(results[\"samples\"][\"log_taus\"]))\n",
    "# plt.imshow(connection_data[\"F\"])\n",
    "plt.plot(connection_data[\"F\"][:,0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa7646fadbe57c277d76ff36de8f181b6360ae43924dda8e36f9c735fd43eb1d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit ('JaiYuLab': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
