{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hddCRP.simulations\n",
    "import hddCRP.modelFitting\n",
    "import hddCRP.behaviorDataHandlers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_directory = \"Results/Simulations\"\n",
    "if(not os.path.exists(results_directory)):\n",
    "    os.makedirs(results_directory)\n",
    "    \n",
    "simulation_id = 0; # for naming file\n",
    "\n",
    "overwrite_existing_results = False\n",
    "\n",
    "num_runs = 2;\n",
    "num_warmup_samples = 1000\n",
    "num_samples = 4000\n",
    "max_blocks_per_type = 10;\n",
    "\n",
    "initialize_fit_with_real_connections = False;\n",
    "\n",
    "depth  = 3; # look 2 actions in the past\n",
    "alphas = [2,10,20] # concentration parameters: per depth in the hddCRP tree. alphas[0] first level (no action context), alphas[1] is the second (for regularizing p(y_t | y_{t-1})), etc...\n",
    "between_session_time_constants = np.array([[ 5, 2],\n",
    "                                            [2, 5]]) # units = sessions\n",
    "within_session_time_constant = 25 # units = actions\n",
    "true_parameters = {\"within_session_time_constant\" : within_session_time_constant,\n",
    "                   \"A_to_A_session_time_constant\" : between_session_time_constants[0,0],\n",
    "                   \"A_to_B_session_time_constant\" : between_session_time_constants[0,1],\n",
    "                   \"B_to_B_session_time_constant\" : between_session_time_constants[1,1],\n",
    "                   \"alpha_concentration_no_context\" : alphas[0],\n",
    "                   \"alpha_concentration_one_back_context\" : alphas[1],\n",
    "                   \"alpha_concentration_two_back_context\" : alphas[2]}\n",
    "session_length = 60 # trials per session\n",
    "action_labels = [0,1,2] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m tau_names \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(xx) \u001b[39mfor\u001b[39;00m xx \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mweight_param_labels]\n\u001b[0;32m     23\u001b[0m alphas_names \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39malpha_concentration_no_context\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39malpha_concentration_one_back_context\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39malpha_concentration_two_back_context\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> 24\u001b[0m model, samples, step_size_settings \u001b[39m=\u001b[39m hddCRP\u001b[39m.\u001b[39;49mbehaviorDataHandlers\u001b[39m.\u001b[39;49msample_model_for_maze_data(model, num_samples\u001b[39m=\u001b[39;49mnum_samples, num_warmup_samples\u001b[39m=\u001b[39;49mnum_warmup_samples)\n\u001b[0;32m     26\u001b[0m MCMC_info \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39minitialized_with_true_connections\u001b[39m\u001b[39m\"\u001b[39m : initialize_fit_with_real_connections,\n\u001b[0;32m     27\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mstep_size_settings\u001b[39m\u001b[39m\"\u001b[39m : step_size_settings\u001b[39m.\u001b[39mto_dict(),\n\u001b[0;32m     28\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnum_warmup_samples\u001b[39m\u001b[39m\"\u001b[39m : num_warmup_samples,\n\u001b[0;32m     29\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnum_samples\u001b[39m\u001b[39m\"\u001b[39m : num_samples}\n\u001b[0;32m     30\u001b[0m samples[\u001b[39m\"\u001b[39m\u001b[39mtau_parameter_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tau_names\n",
      "File \u001b[1;32m~\\gitCode\\hddCRP\\src\\hddCRP\\behaviorDataHandlers.py:306\u001b[0m, in \u001b[0;36msample_model_for_maze_data\u001b[1;34m(hddcrp, num_samples, num_warmup_samples)\u001b[0m\n\u001b[0;32m    299\u001b[0m samples \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mlog_acceptance_probability\u001b[39m\u001b[39m\"\u001b[39m : np\u001b[39m.\u001b[39mzeros((num_samples_total)),\n\u001b[0;32m    300\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39maccepted\u001b[39m\u001b[39m\"\u001b[39m : np\u001b[39m.\u001b[39mzeros((num_samples_total),dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m),\n\u001b[0;32m    301\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39malphas\u001b[39m\u001b[39m\"\u001b[39m   : np\u001b[39m.\u001b[39mzeros((num_samples_total,hddcrp\u001b[39m.\u001b[39malpha\u001b[39m.\u001b[39msize)),\n\u001b[0;32m    302\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mlog_taus\u001b[39m\u001b[39m\"\u001b[39m : np\u001b[39m.\u001b[39mzeros((num_samples_total,hddcrp\u001b[39m.\u001b[39mweight_params\u001b[39m.\u001b[39msize)),\n\u001b[0;32m    303\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mnum_warmup_samples\u001b[39m\u001b[39m\"\u001b[39m : num_warmup_samples}\n\u001b[0;32m    305\u001b[0m \u001b[39mfor\u001b[39;00m ss \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_samples_total):\n\u001b[1;32m--> 306\u001b[0m     hddcrp\u001b[39m.\u001b[39;49mrun_gibbs_sweep()\n\u001b[0;32m    307\u001b[0m     sigma2 \u001b[39m=\u001b[39m step_size_settings\u001b[39m.\u001b[39mstep_size_fixed \u001b[39mif\u001b[39;00m ss \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m num_warmup_samples \u001b[39melse\u001b[39;00m step_size_settings\u001b[39m.\u001b[39mstep_size_for_warmup\n\u001b[0;32m    308\u001b[0m     hddcrp, samples[\u001b[39m\"\u001b[39m\u001b[39mlog_acceptance_probability\u001b[39m\u001b[39m\"\u001b[39m][ss], samples[\u001b[39m\"\u001b[39m\u001b[39maccepted\u001b[39m\u001b[39m\"\u001b[39m][ss] \u001b[39m=\u001b[39m Metropolis_Hastings_step_for_maze_data(hddcrp, sigma2)\n",
      "File \u001b[1;32m~\\gitCode\\hddCRP\\src\\hddCRP\\modelFitting.py:1014\u001b[0m, in \u001b[0;36mhddCRPModel.run_gibbs_sweep\u001b[1;34m(self, order, rng, DEBUG_MODE)\u001b[0m\n\u001b[0;32m   1012\u001b[0m codes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((order\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)\n\u001b[0;32m   1013\u001b[0m \u001b[39mfor\u001b[39;00m ii \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(order\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m-> 1014\u001b[0m     codes[order[ii,\u001b[39m0\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gibbs_sample_single_node(order[ii,\u001b[39m0\u001b[39;49m], order[ii,\u001b[39m1\u001b[39;49m], rng\u001b[39m=\u001b[39;49mrng, DEBUG_MODE\u001b[39m=\u001b[39;49mDEBUG_MODE);\n\u001b[0;32m   1015\u001b[0m \u001b[39mreturn\u001b[39;00m (codes, order)\n",
      "File \u001b[1;32m~\\gitCode\\hddCRP\\src\\hddCRP\\modelFitting.py:1167\u001b[0m, in \u001b[0;36mhddCRPModel._gibbs_sample_single_node\u001b[1;34m(self, node, layer, rng, DEBUG_MODE)\u001b[0m\n\u001b[0;32m   1164\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_CB_num_labeled_in_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_C_num_labeled_in_table\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m   1165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_CB_num_labeled_upstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_C_num_labeled_upstream\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m-> 1167\u001b[0m can_connect_to, post, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post_for_single_nodes_connections(node, layer);\n\u001b[0;32m   1168\u001b[0m node_to \u001b[39m=\u001b[39m rng\u001b[39m.\u001b[39mchoice(can_connect_to, p \u001b[39m=\u001b[39m post\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39msum(post))\n\u001b[0;32m   1170\u001b[0m connection_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_connection(node, node_to, layer)\n",
      "File \u001b[1;32m~\\gitCode\\hddCRP\\src\\hddCRP\\modelFitting.py:1126\u001b[0m, in \u001b[0;36mhddCRPModel._post_for_single_nodes_connections\u001b[1;34m(self, node, layer, print_msgs)\u001b[0m\n\u001b[0;32m   1124\u001b[0m \u001b[39m# gets the log prior probability of each connection (up to a constant)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m p_C \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_F[node,can_connect_to]\n\u001b[1;32m-> 1126\u001b[0m p_C[can_connect_to \u001b[39m==\u001b[39m node] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha[layer];\n\u001b[0;32m   1127\u001b[0m \u001b[39m#log_p_C = np.log(p_C);\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \n\u001b[0;32m   1129\u001b[0m \u001b[39m# gets log posterior probability\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1133\u001b[0m \n\u001b[0;32m   1134\u001b[0m \u001b[39m# gets  posterior probability\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m post \u001b[39m=\u001b[39m p_Y \u001b[39m*\u001b[39m p_C\n",
      "File \u001b[1;32m~\\gitCode\\hddCRP\\src\\hddCRP\\modelFitting.py:431\u001b[0m, in \u001b[0;36mhddCRPModel.alpha\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mall(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_F \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39minvalid connection weights found! check the weight/distance function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    430\u001b[0m \u001b[39m# the alpha values: weight of self/upwards connections for each layer\u001b[39;00m\n\u001b[1;32m--> 431\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    432\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39malpha\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ArrayLike:\n\u001b[0;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_alpha;\n\u001b[0;32m    434\u001b[0m \u001b[39m@alpha\u001b[39m\u001b[39m.\u001b[39msetter\n\u001b[0;32m    435\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39malpha\u001b[39m(\u001b[39mself\u001b[39m, aa : \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m ArrayLike) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for run_idx in range(num_runs):\n",
    "    for N_blocks_per_type in range(2,max_blocks_per_type+1):\n",
    "        filename = \"{results_directory}/Sim_{sim_num}_size_{N_blocks_per_type}_run_{run_idx}.pkl\".format(results_directory=results_directory, sim_num=simulation_id, N_blocks_per_type=N_blocks_per_type, run_idx=run_idx)\n",
    "        if(not os.path.isfile(filename) or overwrite_existing_results):\n",
    "            rng_seed_sim = N_blocks_per_type + 100 + 10000*run_idx;\n",
    "            rng_seed_fit = N_blocks_per_type + 200 + 10000*run_idx;\n",
    "            rng_sim = np.random.Generator(np.random.MT19937(rng_seed_sim))\n",
    "            rng_fit = np.random.Generator(np.random.MT19937(rng_seed_fit))\n",
    "\n",
    "            session_lengths = [session_length] * (2 * N_blocks_per_type)\n",
    "            session_labels = (['A'] * N_blocks_per_type) + (['B'] * N_blocks_per_type) # which maze\n",
    "\n",
    "            seqs, connection_data = hddCRP.simulations.simulate_sequential_hddCRP(session_lengths, session_labels, action_labels, depth, rng_sim, alphas, between_session_time_constants, within_session_time_constant)\n",
    "\n",
    "\n",
    "            simulation_info = {\"rng_seed_simulation\" : rng_seed_sim, \"rng_seed_fitting\" : rng_seed_fit, \"rng_type\" : \"MT19937\",\n",
    "                            \"session_lengths\" : session_lengths, \"session_labels\" : session_labels, \"action_labels\" : action_labels,\n",
    "                            \"seqs\" : seqs, \"connection_data\" : connection_data}\n",
    "\n",
    "            model = hddCRP.simulations.create_hddCRPModel_from_simulated_sequential_hddCRP(seqs, connection_data, rng=rng_fit, use_real_connections=initialize_fit_with_real_connections)\n",
    "            \n",
    "            tau_names = [str(xx) for xx in model.weight_param_labels]\n",
    "            alphas_names = [\"alpha_concentration_no_context\", \"alpha_concentration_one_back_context\", \"alpha_concentration_two_back_context\"]\n",
    "            model, samples, step_size_settings = hddCRP.behaviorDataHandlers.sample_model_for_maze_data(model, num_samples=num_samples, num_warmup_samples=num_warmup_samples)\n",
    "\n",
    "            MCMC_info = {\"initialized_with_true_connections\" : initialize_fit_with_real_connections,\n",
    "                        \"step_size_settings\" : step_size_settings.to_dict(),\n",
    "                        \"num_warmup_samples\" : num_warmup_samples,\n",
    "                        \"num_samples\" : num_samples}\n",
    "            samples[\"tau_parameter_names\"] = tau_names\n",
    "            samples[\"alphas_names\"] = alphas_names\n",
    "            \n",
    "            # save results to filename\n",
    "            with open(filename, \"wb\") as results_file:\n",
    "                results_data = {\"true_parameters\" : true_parameters,\n",
    "                                \"simulation_info\" : simulation_info,\n",
    "                                \"MCMC_info\" : MCMC_info,\n",
    "                                \"samples\" : samples}\n",
    "                pickle.dump(results_data, results_file)\n",
    "            # \n",
    "            #   true_parameters\n",
    "            #   simulation_info\n",
    "            #   MCMC_info\n",
    "            #   samples\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_data = np.zeros((num_samples, num_runs*len(true_parameters)*(max_blocks_per_type-1)))\n",
    "empty_data.fill(np.nan)\n",
    "parameters = pd.DataFrame(empty_data,\n",
    "                  columns=pd.MultiIndex.from_product([true_parameters.keys(), range(2,max_blocks_per_type+1), range(num_runs)], names=[\"parameter\", \"sessions per maze\", \"run\"]))\n",
    "parameters.index.name='sample'\n",
    "\n",
    "# plot results: MCMC credible intervals for each parameter as a function of N_blocks_per_type\n",
    "print(\"Metropolis-Hastings acceptance rate and step size:\")\n",
    "for run_idx in range(num_runs):\n",
    "    for N_blocks_per_type in range(2,max_blocks_per_type+1):\n",
    "        filename = \"{results_directory}/Sim_{sim_num}_size_{N_blocks_per_type}_run_{run_idx}.pkl\".format(results_directory=results_directory, sim_num=simulation_id, N_blocks_per_type=N_blocks_per_type, run_idx=run_idx)\n",
    "        if(not os.path.isfile(filename)):\n",
    "            raise RuntimeError(\"Results file not found: \" + filename)\n",
    "            # throw error\n",
    "        \n",
    "        # load file\n",
    "        with open(filename, \"rb\") as results_file:\n",
    "            results = pickle.load(results_file)\n",
    "\n",
    "            '''\n",
    "            Make sure all the files loaded are correct\n",
    "            '''\n",
    "\n",
    "            # check for correct results fields\n",
    "            expected_fields = [\"true_parameters\", \"simulation_info\", \"MCMC_info\", \"samples\"]\n",
    "            if(not isinstance(results, dict) or not np.all(np.isin(expected_fields, list(results.keys())))):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected results fields\")\n",
    "            \n",
    "            session_lengths = [session_length] * (2 * N_blocks_per_type)\n",
    "            session_labels = (['A'] * N_blocks_per_type) + (['B'] * N_blocks_per_type) # which maze\n",
    "            if(results[\"simulation_info\"][\"session_lengths\"] != session_lengths):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected session lengths\")\n",
    "            if(results[\"simulation_info\"][\"session_labels\"] != session_labels):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected session labels\")\n",
    "            if(results[\"simulation_info\"][\"action_labels\"] != action_labels):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected action labels\")\n",
    "\n",
    "\n",
    "            # check if samples matches (if greater than expected, raise a warning; it less, raise an error)\n",
    "            expected_fields_samples = [\"log_taus\", \"alphas\", \"accepted\", \"num_warmup_samples\", \"tau_parameter_names\", \"alphas_names\"]\n",
    "            if(not isinstance(results[\"samples\"], dict) or not np.all(np.isin(expected_fields_samples, list(results[\"samples\"].keys())))):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected results['samples'] fields\")\n",
    "            \n",
    "            num_samples_found = min(results[\"samples\"][\"log_taus\"].shape[0], results[\"samples\"][\"alphas\"].shape[0]) - results[\"samples\"][\"num_warmup_samples\"]\n",
    "            if(num_samples_found < num_samples):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected number of samples in results['samples']. Found \" + str(num_samples_found) + \". Expected \" + str(num_samples) + \".\")\n",
    "\n",
    "            # check if true params match, if not, raise error\n",
    "            if(not np.all(np.isin(list(true_parameters.keys()), results[\"samples\"][\"tau_parameter_names\"] + results[\"samples\"][\"alphas_names\"]))):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected parameters in results['samples']\")\n",
    "            if(not np.all(np.isin(list(true_parameters.keys()), list(results[\"true_parameters\"].keys()))) or not np.all(np.isin(list(results[\"true_parameters\"].keys()), list(true_parameters.keys())))):\n",
    "                raise ValueError(\"File \" + filename + \" does not contain expected parameters in results['true_parameters']\")\n",
    "            \n",
    "            for p_name in true_parameters.keys():\n",
    "                if(true_parameters[p_name] != results[\"true_parameters\"][p_name]):\n",
    "                    raise ValueError(\"File \" + filename + \" does not contain expected parameter value results['true_parameters'][\" + p_name + \"]\")\n",
    "\n",
    "            # load samples from each parameter into dataframe (multi-index: name of param and run number)\n",
    "            s_index = range(results[\"samples\"][\"num_warmup_samples\"], results[\"samples\"][\"num_warmup_samples\"] + num_samples)\n",
    "            for ii, p_name in enumerate(results[\"samples\"][\"tau_parameter_names\"] ):\n",
    "                \n",
    "                parameters.loc[:,(p_name,N_blocks_per_type,run_idx)] = np.exp(results[\"samples\"][\"log_taus\"][s_index,ii])\n",
    "            for ii, p_name in enumerate(results[\"samples\"][\"alphas_names\"] ):\n",
    "                parameters.loc[:,(p_name,N_blocks_per_type,run_idx)] = results[\"samples\"][\"alphas\"][s_index,ii]\n",
    "\n",
    "            print(\"Run \" + str(run_idx) + \", num blocks \" + str(N_blocks_per_type) + \": rate = \" + str(np.mean(results[\"samples\"][\"accepted\"][s_index])) + \", step size = \" + str(np.sqrt(results[\"MCMC_info\"][\"step_size_settings\"][\"step_size_fixed\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_s = parameters.stack([1,2]).reset_index()    \n",
    "n_cols = 2\n",
    "n_rows = int(np.ceil(len(true_parameters)/float(n_cols)))\n",
    "plt.figure(figsize=(n_cols*8,n_rows*4))\n",
    "\n",
    "print(len(true_parameters))\n",
    "for ii, p_name in enumerate(true_parameters.keys()):\n",
    "    plt.subplot(n_rows, n_cols, ii+1)\n",
    "    # for each parameter in the data frame, make a plot of 95% CI and mean over time vs. true params\n",
    "    pp = sns.lineplot(x=\"sessions per maze\", y=p_name,\n",
    "                hue=\"run\", errorbar=(\"pi\",95),\n",
    "                data=parameters_s)\n",
    "    plt.plot([2,max_blocks_per_type], [true_parameters[p_name], true_parameters[p_name]], \"k:\")\n",
    "    pp.set_xticks(range(2,max_blocks_per_type+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(parameters[\"within_session_time_constant\"][2][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa7646fadbe57c277d76ff36de8f181b6360ae43924dda8e36f9c735fd43eb1d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit ('JaiYuLab': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
